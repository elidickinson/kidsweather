# ==================================================
# WEATHER API CONFIGURATION (Required)
# ==================================================

# Get your free API key at: https://openweathermap.org/api
WEATHER_API_KEY=your_openweathermap_api_key_here

# ==================================================
# LLM CONFIGURATION (Required - Choose One Provider)
# ==================================================

# All three LLM settings below are required for the service to work
# Choose one of the provider configurations below and uncomment it

# ------------------ DeepSeek ------------------
# Fast, cheap, and high quality. Get API key at: https://platform.deepseek.com/
# LLM_API_URL=https://api.deepseek.com/chat/completions
# LLM_API_KEY=sk-your-deepseek-api-key
# LLM_MODEL=deepseek-chat

# ------------------ OpenAI ------------------
# Get API key at: https://platform.openai.com/
# LLM_API_URL=https://api.openai.com/v1/chat/completions
# LLM_API_KEY=sk-your-openai-api-key
# LLM_MODEL=gpt-3.5-turbo
# Alternative models: gpt-4, gpt-4-turbo

# ------------------ OpenRouter ------------------
# Access to many models via one API. Get key at: https://openrouter.ai/
# LLM_API_URL=https://openrouter.ai/api/v1/chat/completions
# LLM_API_KEY=sk-or-v1-your-openrouter-key
# LLM_MODEL=anthropic/claude-3-haiku
# Alternative models: meta-llama/llama-3.1-8b-instruct, google/gemma-2-9b-it

# ------------------ Anthropic (via OpenRouter) ------------------
# LLM_API_URL=https://openrouter.ai/api/v1/chat/completions
# LLM_API_KEY=sk-or-v1-your-openrouter-key
# LLM_MODEL=anthropic/claude-3-haiku

# ------------------ LM Studio (Local) ------------------
# Run models locally with LM Studio. Download at: https://lmstudio.ai/
# LLM_API_URL=http://localhost:1234/v1/chat/completions
# LLM_API_KEY=lm-studio
# LLM_MODEL=your-downloaded-model-name
# LLM_SUPPORTS_JSON_MODE=false

# ------------------ Ollama (Local) ------------------
# Run models locally with Ollama. Download at: https://ollama.ai/
# LLM_API_URL=http://localhost:11434/v1/chat/completions
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.2:3b

# ==================================================
# OPTIONAL: JSON MODE SUPPORT
# ==================================================

# Most major providers support JSON mode, but some local setups don't
# Set to false if you get LLM API errors
LLM_SUPPORTS_JSON_MODE=true

# ==================================================
# OPTIONAL: FALLBACK LLM CONFIGURATION
# ==================================================

# If your primary LLM fails, automatically try a backup
# All four settings below must be configured for fallback to work
# Example: Use DeepSeek as primary, OpenRouter as fallback

# FALLBACK_LLM_API_URL=https://openrouter.ai/api/v1/chat/completions
# FALLBACK_LLM_API_KEY=sk-or-v1-your-openrouter-key
# FALLBACK_LLM_MODEL=anthropic/claude-3-haiku
# FALLBACK_LLM_SUPPORTS_JSON_MODE=true

# ==================================================
# OPTIONAL: ADVANCED SETTINGS
# ==================================================

# API cache time in seconds (default: 600 = 10 minutes)
# API_CACHE_TIME=600

# Default system prompt file (default: prompts/default.txt)
# DEFAULT_PROMPT_FILE=prompts/default.txt

# Weather API units (default: imperial for Fahrenheit)
# WEATHER_UNITS=imperial

# ==================================================
# SETUP INSTRUCTIONS
# ==================================================

# 1. Copy this file to .env: cp .env.example .env
# 2. Get your OpenWeatherMap API key (free): https://openweathermap.org/api
# 3. Choose and configure ONE LLM provider above
# 4. Uncomment the three lines for your chosen provider
# 5. Replace the placeholder values with your actual API keys
# 6. Save the file and run the application

# Testing your setup:
# uv run ./weather_cli.py --lat 38.9 --lon -77.0
